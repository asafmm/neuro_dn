{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_stimuli_json(subject):\n",
    "    with open(f'data/{subject}/stimuli.js', 'r') as f:\n",
    "        stimuli_js = f.read()\n",
    "    stimuli_json_str = stimuli_js[20:]\n",
    "    with open(f'data/{subject}/stimuli.json', 'w') as f:\n",
    "        f.write(stimuli_json_str)\n",
    "    stimuli_json = pd.read_json(f'data/{subject}/stimuli.json')\n",
    "    stimuli_json.loc[:, 'triad_index'] = np.arange(1, len(stimuli_json)+1, dtype=int)\n",
    "    return stimuli_json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_triad_index_from_targets(stimuli_df, stimuli_json):\n",
    "    stimuli_df_no_blanks = stimuli_df[stimuli_df.target1_rank.notnull()]\n",
    "    stimuli_df_only_blanks = stimuli_df[stimuli_df.target1==0]\n",
    "    stimuli_json_no_blanks = stimuli_json[stimuli_json.target1_rank.notnull()]\n",
    "    triad_index_dict = {(int(target1), int(target2), int(distractor)):int(triad_index) for i, \n",
    "                    (triad_index, target1, target2, distractor) in \n",
    "                    stimuli_json_no_blanks[['triad_index', 'target1_rank', 'target2_rank', 'distractor_rank']].iterrows()}\n",
    "    triad_index_dict[(0, 0, 0)] = len(stimuli_json_no_blanks) + 1\n",
    "    # blank_id = 0\n",
    "    for i, row in stimuli_df.iterrows():\n",
    "        indices = row[['target1_rank', 'target2_rank', 'distractor_rank']]\n",
    "        if all(indices.notna()):\n",
    "            indices_tuple = tuple(indices.values)\n",
    "            stimuli_df.loc[i, 'triad_index'] = triad_index_dict[indices_tuple]\n",
    "        else:\n",
    "            stimuli_df.loc[i, 'triad_index'] = triad_index_dict[(0, 0, 0)] # + blank_id\n",
    "            # blank_id += 1\n",
    "    stimuli_df.triad_index = stimuli_df.triad_index.astype(int)\n",
    "    return stimuli_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_durations_multiple_files(file_path):\n",
    "    run_df = pd.read_csv(file_path)\n",
    "    responses_df = run_df[run_df.part=='trinary']\n",
    "    responses_df = responses_df[['triad_index', 'response']]\n",
    "    # nas_responses = responses_df[responses_df.response.isna()]\n",
    "    responses_df = responses_df.dropna()\n",
    "\n",
    "    scan_start = run_df.index[run_df.part=='scan_wait'][0]\n",
    "    run_df = run_df.iloc[scan_start:, :] # filter only scan part\n",
    "    run_durations = np.diff(run_df.time_elapsed)/1000\n",
    "    run_df.loc[:, 'duration'] = np.append(0, run_durations)\n",
    "    elapsed_time = run_df.duration.cumsum()[:-1]\n",
    "    run_df.loc[:, 'run_elapsed_time'] = np.append(0, elapsed_time)\n",
    "    ev_times = run_df.loc[run_df.ID.notnull(), ['run_elapsed_time', 'duration', 'triad_index', \n",
    "                                                'response', 'rt']].reset_index(drop=True)\n",
    "    ev_times.loc[:, 'ID'] = ev_times.ID.astype(int)\n",
    "    \n",
    "\n",
    "    return ev_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_run_df(file_path):\n",
    "    run_df = pd.read_csv(file_path)\n",
    "    scan_start_time = run_df.loc[run_df.part=='scan_wait', 'time_elapsed']\n",
    "    responses_df = run_df[run_df.part=='evaluation']\n",
    "    responses_df = responses_df[['ID', 'time_elapsed', 'response', 'rt', 'prob', 'amount']]\n",
    "    responses_df.loc[:, 'time_elapsed'] = responses_df.time_elapsed - scan_start_time\n",
    "    responses_df.response = responses_df.response.astype(float)\n",
    "    responses_df.rt = responses_df.rt/1000\n",
    "    responses_df.loc[:, 'EV'] = responses_df.prob * responses_df.amount / 100\n",
    "    return responses_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_subject_stats(all_runs, subject_num):\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5), dpi=150)\n",
    "    # choices\n",
    "    axs[0].hist(all_runs.response.dropna(), color='lightcoral')\n",
    "    axs[0].set_title('Choice distribution', fontsize=18)\n",
    "    axs[0].set_xlabel('Choice (NIS)', fontsize=16)\n",
    "    axs[0].set_xlim(-5, 85)\n",
    "    axs[0].tick_params(axis='x', labelsize=13)\n",
    "    axs[0].tick_params(axis='y', labelsize=13)\n",
    "    # RT and misses\n",
    "    n_blank_trials = len(all_runs[all_runs.amount==0])\n",
    "    missed_trials = all_runs.rt.isna().sum() - n_blank_trials\n",
    "    axs[1].hist(all_runs.rt, color='darkseagreen')\n",
    "    axs[1].set_title(f'RT distribution (missed: {missed_trials})', fontsize=18)\n",
    "    axs[1].set_xlabel('RT (s)', fontsize=16)\n",
    "    axs[1].set_xlim(0, 5.5)\n",
    "    axs[1].tick_params(axis='x', labelsize=13)\n",
    "    axs[1].tick_params(axis='y', labelsize=13)\n",
    "    # EV sensitivity\n",
    "    ev_sv_corr = all_runs[['EV', 'response']].corr().iloc[0, 1] # ignore nans\n",
    "    axs[2].scatter(all_runs.EV, all_runs.response, color='steelblue', alpha=0.5)\n",
    "    axs[2].set_title(f'EV sensitivity (r={ev_sv_corr:.2f})', fontsize=18)\n",
    "    axs[2].set_xlabel('EV', fontsize=16)\n",
    "    axs[2].set_ylabel('Choice', fontsize=16)\n",
    "    axs[2].set_ylim(-5, 85)\n",
    "    axs[2].tick_params(axis='x', labelsize=13)\n",
    "    axs[2].tick_params(axis='y', labelsize=13)\n",
    "    plt.suptitle(f'Subject {subject_num}', fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_durations(mri_df, run_i):\n",
    "    # start and end indices of the run\n",
    "    n_runs = len(np.where(mri_df.part=='scan_wait')[0])\n",
    "    run_indices = np.where(mri_df.part=='scan_wait')[0][run_i-1:run_i+1]\n",
    "\n",
    "    if run_i==n_runs:\n",
    "        last_delay_ind = np.where(mri_df.part=='fixation')[0][-1]\n",
    "        run_indices = np.append(run_indices, last_delay_ind)\n",
    "\n",
    "    # only indices of run i\n",
    "    run_df = mri_df.iloc[run_indices[0]:run_indices[1], :].copy()\n",
    "    run_durations = np.diff(run_df.time_elapsed)/1000\n",
    "    run_df.loc[:, 'duration'] = np.append(0, run_durations)\n",
    "    elapsed_time = run_df.duration.cumsum()[:-1]\n",
    "    run_df.loc[:, 'run_elapsed_time'] = np.append(0, elapsed_time)\n",
    "    ev_times = run_df.loc[run_df.ID.notnull(), ['run_elapsed_time', 'duration', 'ID', 'part']].reset_index(drop=True)\n",
    "    ev_times.loc[:, 'ID'] = ev_times.ID.astype(int)\n",
    "\n",
    "    # sum duration of viewing and evaluation task\n",
    "    merged_view_eval_times = pd.DataFrame(columns=ev_times.columns[:-1])\n",
    "    for i in range(0, len(ev_times), 2):\n",
    "        eval_duration = ev_times.loc[i+1, 'duration']\n",
    "        view_duration = ev_times.loc[i, 'duration']\n",
    "        merged_view_eval_times.loc[i, :] = ev_times.loc[i, :]\n",
    "        merged_view_eval_times.loc[i, 'duration'] = view_duration + eval_duration\n",
    "    merged_view_eval_times = merged_view_eval_times.reset_index(drop=True)\n",
    "    merged_view_eval_times.loc[:, 'ev'] = np.arange(1, len(merged_view_eval_times)+1)\n",
    "\n",
    "    return ev_times, merged_view_eval_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_evs_to_txt(merged_view_eval_times, run_i, fsf_dir):\n",
    "    \"\"\"\n",
    "    Write EVs to txt files for each run\n",
    "    \"\"\"\n",
    "    i = 1\n",
    "    triad_indices = merged_view_eval_times.triad_index.unique()\n",
    "    for triad_index in triad_indices:\n",
    "        triad_df = merged_view_eval_times[merged_view_eval_times.triad_index==triad_index]\n",
    "        if triad_index == max(triad_indices):\n",
    "            # blank trials\n",
    "            start_str = [str(np.round(run_elapsed_time, 3)) for run_elapsed_time in triad_df.time]\n",
    "            duration_str = [str(np.round(duration, 3)) for duration in triad_df.duration]\n",
    "            ev_string = [' '.join([start_str, duration_str, '1']) for start_str, duration_str in zip(start_str, duration_str)]\n",
    "            ev_string = '\\n'.join(ev_string)\n",
    "        else:\n",
    "            elapsed_str = str(merged_view_eval_times.loc[i, 'time'])\n",
    "            duration_str = str(merged_view_eval_times.loc[i, 'duration'])\n",
    "            ev_string = ' '.join([elapsed_str, duration_str, '1'])        \n",
    "        with open(fsf_dir + f'/run{run_i}/evs/run{run_i}_ev{i}.txt', 'w') as f:\n",
    "            f.write(ev_string) \n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_evs_to_txt_unify_blanks(merged_view_eval_times, run_i, fsf_dir):\n",
    "    \"\"\"\n",
    "    Write EVs to txt files for each run\n",
    "    \"\"\"\n",
    "    i = 1\n",
    "    for lottery_id in merged_view_eval_times.ID.unique():\n",
    "        lottery_df = merged_view_eval_times[merged_view_eval_times.ID==lottery_id] \n",
    "        if lottery_id == 0:\n",
    "            start_str = [str(np.round(run_elapsed_time, 3)) for run_elapsed_time in lottery_df.run_elapsed_time]\n",
    "            duration_str = [str(np.round(duration, 3)) for duration in lottery_df.duration]\n",
    "            ev_string = [' '.join([start_str, duration_str, '1']) for start_str, duration_str in zip(start_str, duration_str)]\n",
    "            ev_string = '\\n'.join(ev_string)\n",
    "        else:\n",
    "            start_str = str(np.round(lottery_df.run_elapsed_time.values[0], 3))\n",
    "            duration_str = str(np.round(lottery_df.duration.values[0], 3))\n",
    "            ev_string = ' '.join([start_str, duration_str, '1'])\n",
    "        with open(fsf_dir + f'/run{run_i}/evs_unified_blanks/run{run_i}_ev{i}.txt', 'w') as f:\n",
    "            f.write(ev_string)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_evs_to_txt_unify_view(unified_trials, run_i, fsf_dir):\n",
    "    \"\"\"\n",
    "    Write EVs to txt files for each run\n",
    "    \"\"\"\n",
    "    i = 1\n",
    "    for lottery_id in unified_trials.ID.unique():\n",
    "        lottery_df = unified_trials[unified_trials.ID==lottery_id] \n",
    "        if lottery_id == 0:\n",
    "            start_str = [str(np.round(view_time, 3)) for view_time in lottery_df.view_time]\n",
    "            duration_str = [str(np.round(rt + 1, 3)) for rt in lottery_df.rt]\n",
    "            ev_string = [' '.join([start_str, duration_str, '1']) for start_str, duration_str in zip(start_str, duration_str)]\n",
    "            ev_string = '\\n'.join(ev_string)\n",
    "        else:\n",
    "            start_str = str(np.round(lottery_df.view_time.values[0], 3))\n",
    "            # duration_str = str(np.round(lottery_df.view_duration.values[0], 3))\n",
    "            duration_str = \"1\"\n",
    "            ev_string = ' '.join([start_str, duration_str, '1'])\n",
    "        with open(fsf_dir + f'/run{run_i}/view/evs_view/run{run_i}_ev{i}.txt', 'w') as f:\n",
    "            f.write(ev_string)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which EVs to average?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing run 4...\n",
      "Processing run 2...\n",
      "Processing run 1...\n",
      "Processing run 3...\n"
     ]
    }
   ],
   "source": [
    "SUBJECT_NUM = '104'\n",
    "results_dir = f'data/{SUBJECT_NUM}/'\n",
    "results_path = glob.glob(results_dir + '*.csv')\n",
    "ignore_choice_results = ['neuro_DN' not in f for f in results_path]\n",
    "results_path = np.array(results_path)[ignore_choice_results]\n",
    "all_runs = pd.DataFrame()\n",
    "WRITE_EVS = True\n",
    "df_list = []\n",
    "stimuli_json = read_stimuli_json(SUBJECT_NUM)\n",
    "if WRITE_EVS:\n",
    "    fsf_dir = f'/Volumes/homes/Asaf/qunex/neuro_DN/task_analysis/{SUBJECT_NUM}'\n",
    "    if not os.path.exists(fsf_dir):\n",
    "            os.mkdir(fsf_dir)\n",
    "ev_ids = []\n",
    "for f in results_path:\n",
    "    # extract stimuli ids\n",
    "    regex_files = re.findall(r\"/(\\d)_\", f)\n",
    "    if len(regex_files) > 0:\n",
    "        run_i = re.findall(r\"/(\\d)_\", f)[0]\n",
    "    else:\n",
    "        run_i = 1\n",
    "    print(f'Processing run {run_i}...')\n",
    "    this_run_df = pd.read_csv(f)\n",
    "    scan_start = this_run_df.index[this_run_df.part=='scan_wait'][0]\n",
    "    this_run_df = this_run_df.iloc[scan_start:, :] # filter only scan part\n",
    "    this_run_df.loc[:, 'duration'] = this_run_df.time_elapsed.diff() / 1000\n",
    "    this_run_df.loc[:, 'rt'] = this_run_df.rt / 1000\n",
    "    times = this_run_df.duration.cumsum()[:-1].fillna(0)\n",
    "    times = np.append(np.nan, times)\n",
    "    this_run_df.loc[:, 'time'] = times\n",
    "    # remove all fixation/non-trials\n",
    "    this_run_df = this_run_df[this_run_df.target1.notnull()]\n",
    "    # add triad index, which are missing from pilot subjects...\n",
    "    this_run_df = add_triad_index_from_targets(this_run_df, stimuli_json)\n",
    "    this_run_df.loc[:, 'run'] = int(run_i) \n",
    "    # reset index for trial order and number\n",
    "    this_run_df.reset_index(drop=True, inplace=True)\n",
    "    if WRITE_EVS:\n",
    "        if not os.path.isdir(fsf_dir + f'/run{run_i}/'):\n",
    "            os.mkdir(fsf_dir + f'/run{run_i}/')    \n",
    "        if not os.path.isdir(fsf_dir + f'/run{run_i}/evs/'):\n",
    "            os.mkdir(fsf_dir + f'/run{run_i}/evs/')\n",
    "        write_evs_to_txt(this_run_df, run_i, fsf_dir)\n",
    "    trial_order = np.arange(1, len(this_run_df)+1)\n",
    "    temp_ev_id = pd.DataFrame({f'stimulus_id':this_run_df.triad_index, f'ev_num_run{run_i}':trial_order})\n",
    "    temp_ev_id = temp_ev_id.drop_duplicates('stimulus_id').reset_index(drop=True)\n",
    "    temp_ev_id.loc[:, f'ev_num_run{run_i}'] = np.arange(1, len(temp_ev_id)+1)\n",
    "    ev_ids.append(temp_ev_id)\n",
    "    df_list.append(this_run_df)\n",
    "all_runs = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ev_num_run4</th>\n",
       "      <th>ev_num_run2</th>\n",
       "      <th>ev_num_run1</th>\n",
       "      <th>ev_num_run3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stimulus_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>33</td>\n",
       "      <td>18</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36</td>\n",
       "      <td>11</td>\n",
       "      <td>24</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>28</td>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20</td>\n",
       "      <td>34</td>\n",
       "      <td>31</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22</td>\n",
       "      <td>10</td>\n",
       "      <td>28</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>33</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>32</td>\n",
       "      <td>24</td>\n",
       "      <td>12</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>16</td>\n",
       "      <td>23</td>\n",
       "      <td>19</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>35</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>27</td>\n",
       "      <td>21</td>\n",
       "      <td>26</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>8</td>\n",
       "      <td>27</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6</td>\n",
       "      <td>26</td>\n",
       "      <td>37</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>37</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>18</td>\n",
       "      <td>20</td>\n",
       "      <td>27</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>17</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>21</td>\n",
       "      <td>32</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>24</td>\n",
       "      <td>31</td>\n",
       "      <td>36</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>26</td>\n",
       "      <td>37</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "      <td>29</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>31</td>\n",
       "      <td>18</td>\n",
       "      <td>34</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>14</td>\n",
       "      <td>25</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>10</td>\n",
       "      <td>36</td>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>28</td>\n",
       "      <td>35</td>\n",
       "      <td>33</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ev_num_run4  ev_num_run2  ev_num_run1  ev_num_run3\n",
       "stimulus_id                                                    \n",
       "1                     12           33           18           25\n",
       "2                     36           11           24           21\n",
       "3                     13           28            8           17\n",
       "4                     11            4           13           32\n",
       "5                      9           19            6           29\n",
       "6                     20           34           31           13\n",
       "7                     22           10           28           10\n",
       "8                     33           22           23            9\n",
       "9                     32           24           12           24\n",
       "10                    16           23           19           14\n",
       "11                    35           13           11           37\n",
       "12                    27           21           26           36\n",
       "13                    30            2           22           18\n",
       "14                     7           17            4           23\n",
       "15                     8           27           16            2\n",
       "16                    15           14           21            3\n",
       "17                     3           16            3           35\n",
       "18                     6           26           37           22\n",
       "19                    37            8           32           28\n",
       "20                    18           20           27           15\n",
       "21                     5           12            2            4\n",
       "22                    29           29           17           33\n",
       "23                    23            9           15            5\n",
       "24                    34            1           35           30\n",
       "25                    21           32           10           11\n",
       "26                     1           30            5           27\n",
       "27                    24           31           36           12\n",
       "28                    19            6            9            1\n",
       "29                    26           37           25           26\n",
       "30                    25           15           29           16\n",
       "31                     4            7            1           20\n",
       "32                    17            3           14            6\n",
       "33                    31           18           34           31\n",
       "34                    14           25           30            8\n",
       "35                    10           36           20           19\n",
       "36                    28           35           33           34\n",
       "37                     2            5            7            7"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_evs = pd.DataFrame({f'stimulus_id':[]})\n",
    "for ev_id_df in ev_ids:\n",
    "    merged_evs = pd.merge(merged_evs, ev_id_df, on='stimulus_id', how='outer')\n",
    "merged_evs_sorted = merged_evs.sort_values('stimulus_id').set_index('stimulus_id')\n",
    "merged_evs_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_evs_sorted.to_csv(f'ev_data/{SUBJECT_NUM}_ev_ids.csv', float_format='%.0f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edit fsf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "WRITE_FSF = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run 5 not available, skipping\n"
     ]
    }
   ],
   "source": [
    "if WRITE_FSF:\n",
    "    fsf_path = 'ev_data/run1_hp200_s4_level1.fsf'\n",
    "    fsf = open(fsf_path, 'r').read()\n",
    "    for run_i in range(1, 6):\n",
    "        fsf_dir = f'/Volumes/homes/Asaf/qunex/neuro_DN/task_analysis/{SUBJECT_NUM}'\n",
    "        new_fsf_path = f'{fsf_dir}/run{run_i}/run{run_i}_hp200_s4_level1.fsf'\n",
    "        new_fsf = fsf\n",
    "        if f'ev_num_run{run_i}' not in merged_evs_sorted.columns:\n",
    "            print(f'run {run_i} not available, skipping')\n",
    "            continue\n",
    "        else:\n",
    "            run_evs = merged_evs_sorted.loc[:, f'ev_num_run{run_i}'].astype(int)\n",
    "        for stimulus_id in run_evs.index:\n",
    "            stimulus_ev = run_evs.loc[stimulus_id]\n",
    "            line_pattern = re.compile(fr'set fmri\\(custom{stimulus_id}\\) \".*txt\"')\n",
    "            line_to_replace = re.findall(line_pattern, new_fsf)[0]\n",
    "            new_line = fr'set fmri(custom{stimulus_id}) \"../evs/run{run_i}_ev{stimulus_ev}.txt\"'\n",
    "            new_fsf = new_fsf.replace(line_to_replace, new_line)\n",
    "            new_fsf = new_fsf.replace(\"MNINonLinear/Results/1/1\", f\"MNINonLinear/Results/{run_i}/{run_i}\")\n",
    "            new_fsf = new_fsf.replace(\"105\", SUBJECT_NUM)\n",
    "        if not os.path.exists(fsf_dir):\n",
    "            os.mkdir(fsf_dir)\n",
    "        open(new_fsf_path, f'w').write(new_fsf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EV_TYPE == 'org_evaluation':\n",
    "    fsf_path = '/Volumes/homes/Asaf/qunex/decoy/task_analysis/001/design_run1_hp200_s4_level1.fsf'\n",
    "    fsf = open(fsf_path, 'r').read()\n",
    "    for run_i in range(1, 6):\n",
    "        fsf_dir = f'/Volumes/homes/Asaf/qunex/decoy/task_analysis/{SUBJECT_NUM}'\n",
    "        new_fsf_path = f'{fsf_dir}/design_run{run_i}_hp200_s4_level1.fsf'\n",
    "        new_fsf = fsf\n",
    "        run_evs = merged_evs_sorted.loc[:, f'ev_num_run{run_i}']\n",
    "        for stimulus_id in run_evs.index:\n",
    "            stimulus_ev = run_evs.loc[stimulus_id]\n",
    "            line_pattern = re.compile(f'set fmri\\(custom{stimulus_id}\\) \".*txt\"')\n",
    "            line_to_replace = re.findall(line_pattern, new_fsf)[0]\n",
    "            new_line = f'set fmri(custom{stimulus_id}) \"../evs/run{run_i}_ev{stimulus_ev}.txt\"'\n",
    "            new_fsf = new_fsf.replace(line_to_replace, new_line)\n",
    "            new_fsf = new_fsf.replace(\"MNINonLinear/Results/1/1\", f\"MNINonLinear/Results/{run_i}/{run_i}\")\n",
    "            new_fsf = new_fsf.replace(\"001\", SUBJECT_NUM)\n",
    "        if not os.path.exists(fsf_dir):\n",
    "            os.mkdir(fsf_dir)\n",
    "        open(new_fsf_path, 'w').write(new_fsf)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('fc_inconsistency')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6a39f30071f3726ae53430eaa9d7338beee3bafecdeac70059570bf43fb11a30"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
